{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Exercise 4.b  multiclass classification using distilled bert.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU","widgets":{"application/vnd.jupyter.widget-state+json":{"d1aa42091cde42539d404e480c82d429":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_6af1354fd22c4484bb3e73d2a26c1c69","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_450a8515e3b3474d88763ee6ed4c3796","IPY_MODEL_18ff56c88f3a4daa8af41bc5d0dd7ba2"]}},"6af1354fd22c4484bb3e73d2a26c1c69":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"450a8515e3b3474d88763ee6ed4c3796":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","state":{"_view_name":"ProgressView","style":"IPY_MODEL_4848994144984a99894c9fbafad80a6f","_dom_classes":[],"description":"Downloading: 100%","_model_name":"FloatProgressModel","bar_style":"success","max":213450,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":213450,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_ff6b0fbb29f040258d884ee1b5b8caef"}},"18ff56c88f3a4daa8af41bc5d0dd7ba2":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","state":{"_view_name":"HTMLView","style":"IPY_MODEL_5f8693fd3f904900b2de98d8f354ab99","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 213k/213k [00:00&lt;00:00, 2.77MB/s]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_b370280b34604d5296073db394a03432"}},"4848994144984a99894c9fbafad80a6f":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"initial","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"ff6b0fbb29f040258d884ee1b5b8caef":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"5f8693fd3f904900b2de98d8f354ab99":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"b370280b34604d5296073db394a03432":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}}}}},"cells":[{"cell_type":"code","metadata":{"id":"K_PJD4iGl-Tl","colab":{"base_uri":"https://localhost:8080/"},"outputId":"680c9b19-76b4-4aea-fe61-90350ab5d169"},"source":["from google.colab import drive\n","drive.mount('/content/gdrive',force_remount=True)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Mounted at /content/gdrive\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"HNboo9EtSQZ3"},"source":["Importing Python Libraries and preparing the environment\n","At this step we will be importing the libraries and modules needed to run our script. Libraries are:\n","\n","* Pandas\n","* Pytorch\n","* Pytorch Utils for Dataset and Dataloader\n","* Transformers\n","* DistilledBERT Model and Tokenizer"]},{"cell_type":"code","metadata":{"id":"A3O0mOvvmi8F","colab":{"base_uri":"https://localhost:8080/"},"outputId":"b5699f4b-a2e7-4eeb-b0c5-bdd5b328be83"},"source":["!pip install transformers"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Collecting transformers\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/2c/4e/4f1ede0fd7a36278844a277f8d53c21f88f37f3754abf76a5d6224f76d4a/transformers-3.4.0-py3-none-any.whl (1.3MB)\n","\r\u001b[K     |▎                               | 10kB 24.2MB/s eta 0:00:01\r\u001b[K     |▌                               | 20kB 11.4MB/s eta 0:00:01\r\u001b[K     |▉                               | 30kB 15.2MB/s eta 0:00:01\r\u001b[K     |█                               | 40kB 15.3MB/s eta 0:00:01\r\u001b[K     |█▎                              | 51kB 11.0MB/s eta 0:00:01\r\u001b[K     |█▋                              | 61kB 11.2MB/s eta 0:00:01\r\u001b[K     |█▉                              | 71kB 10.5MB/s eta 0:00:01\r\u001b[K     |██                              | 81kB 11.5MB/s eta 0:00:01\r\u001b[K     |██▍                             | 92kB 11.3MB/s eta 0:00:01\r\u001b[K     |██▋                             | 102kB 11.0MB/s eta 0:00:01\r\u001b[K     |██▉                             | 112kB 11.0MB/s eta 0:00:01\r\u001b[K     |███▏                            | 122kB 11.0MB/s eta 0:00:01\r\u001b[K     |███▍                            | 133kB 11.0MB/s eta 0:00:01\r\u001b[K     |███▋                            | 143kB 11.0MB/s eta 0:00:01\r\u001b[K     |████                            | 153kB 11.0MB/s eta 0:00:01\r\u001b[K     |████▏                           | 163kB 11.0MB/s eta 0:00:01\r\u001b[K     |████▍                           | 174kB 11.0MB/s eta 0:00:01\r\u001b[K     |████▊                           | 184kB 11.0MB/s eta 0:00:01\r\u001b[K     |█████                           | 194kB 11.0MB/s eta 0:00:01\r\u001b[K     |█████▏                          | 204kB 11.0MB/s eta 0:00:01\r\u001b[K     |█████▌                          | 215kB 11.0MB/s eta 0:00:01\r\u001b[K     |█████▊                          | 225kB 11.0MB/s eta 0:00:01\r\u001b[K     |██████                          | 235kB 11.0MB/s eta 0:00:01\r\u001b[K     |██████▎                         | 245kB 11.0MB/s eta 0:00:01\r\u001b[K     |██████▌                         | 256kB 11.0MB/s eta 0:00:01\r\u001b[K     |██████▊                         | 266kB 11.0MB/s eta 0:00:01\r\u001b[K     |███████                         | 276kB 11.0MB/s eta 0:00:01\r\u001b[K     |███████▎                        | 286kB 11.0MB/s eta 0:00:01\r\u001b[K     |███████▋                        | 296kB 11.0MB/s eta 0:00:01\r\u001b[K     |███████▉                        | 307kB 11.0MB/s eta 0:00:01\r\u001b[K     |████████                        | 317kB 11.0MB/s eta 0:00:01\r\u001b[K     |████████▍                       | 327kB 11.0MB/s eta 0:00:01\r\u001b[K     |████████▋                       | 337kB 11.0MB/s eta 0:00:01\r\u001b[K     |████████▉                       | 348kB 11.0MB/s eta 0:00:01\r\u001b[K     |█████████▏                      | 358kB 11.0MB/s eta 0:00:01\r\u001b[K     |█████████▍                      | 368kB 11.0MB/s eta 0:00:01\r\u001b[K     |█████████▋                      | 378kB 11.0MB/s eta 0:00:01\r\u001b[K     |██████████                      | 389kB 11.0MB/s eta 0:00:01\r\u001b[K     |██████████▏                     | 399kB 11.0MB/s eta 0:00:01\r\u001b[K     |██████████▍                     | 409kB 11.0MB/s eta 0:00:01\r\u001b[K     |██████████▊                     | 419kB 11.0MB/s eta 0:00:01\r\u001b[K     |███████████                     | 430kB 11.0MB/s eta 0:00:01\r\u001b[K     |███████████▏                    | 440kB 11.0MB/s eta 0:00:01\r\u001b[K     |███████████▌                    | 450kB 11.0MB/s eta 0:00:01\r\u001b[K     |███████████▊                    | 460kB 11.0MB/s eta 0:00:01\r\u001b[K     |████████████                    | 471kB 11.0MB/s eta 0:00:01\r\u001b[K     |████████████▎                   | 481kB 11.0MB/s eta 0:00:01\r\u001b[K     |████████████▌                   | 491kB 11.0MB/s eta 0:00:01\r\u001b[K     |████████████▊                   | 501kB 11.0MB/s eta 0:00:01\r\u001b[K     |█████████████                   | 512kB 11.0MB/s eta 0:00:01\r\u001b[K     |█████████████▎                  | 522kB 11.0MB/s eta 0:00:01\r\u001b[K     |█████████████▌                  | 532kB 11.0MB/s eta 0:00:01\r\u001b[K     |█████████████▉                  | 542kB 11.0MB/s eta 0:00:01\r\u001b[K     |██████████████                  | 552kB 11.0MB/s eta 0:00:01\r\u001b[K     |██████████████▍                 | 563kB 11.0MB/s eta 0:00:01\r\u001b[K     |██████████████▋                 | 573kB 11.0MB/s eta 0:00:01\r\u001b[K     |██████████████▉                 | 583kB 11.0MB/s eta 0:00:01\r\u001b[K     |███████████████▏                | 593kB 11.0MB/s eta 0:00:01\r\u001b[K     |███████████████▍                | 604kB 11.0MB/s eta 0:00:01\r\u001b[K     |███████████████▋                | 614kB 11.0MB/s eta 0:00:01\r\u001b[K     |████████████████                | 624kB 11.0MB/s eta 0:00:01\r\u001b[K     |████████████████▏               | 634kB 11.0MB/s eta 0:00:01\r\u001b[K     |████████████████▍               | 645kB 11.0MB/s eta 0:00:01\r\u001b[K     |████████████████▊               | 655kB 11.0MB/s eta 0:00:01\r\u001b[K     |█████████████████               | 665kB 11.0MB/s eta 0:00:01\r\u001b[K     |█████████████████▏              | 675kB 11.0MB/s eta 0:00:01\r\u001b[K     |█████████████████▌              | 686kB 11.0MB/s eta 0:00:01\r\u001b[K     |█████████████████▊              | 696kB 11.0MB/s eta 0:00:01\r\u001b[K     |██████████████████              | 706kB 11.0MB/s eta 0:00:01\r\u001b[K     |██████████████████▎             | 716kB 11.0MB/s eta 0:00:01\r\u001b[K     |██████████████████▌             | 727kB 11.0MB/s eta 0:00:01\r\u001b[K     |██████████████████▊             | 737kB 11.0MB/s eta 0:00:01\r\u001b[K     |███████████████████             | 747kB 11.0MB/s eta 0:00:01\r\u001b[K     |███████████████████▎            | 757kB 11.0MB/s eta 0:00:01\r\u001b[K     |███████████████████▌            | 768kB 11.0MB/s eta 0:00:01\r\u001b[K     |███████████████████▉            | 778kB 11.0MB/s eta 0:00:01\r\u001b[K     |████████████████████            | 788kB 11.0MB/s eta 0:00:01\r\u001b[K     |████████████████████▎           | 798kB 11.0MB/s eta 0:00:01\r\u001b[K     |████████████████████▋           | 808kB 11.0MB/s eta 0:00:01\r\u001b[K     |████████████████████▉           | 819kB 11.0MB/s eta 0:00:01\r\u001b[K     |█████████████████████           | 829kB 11.0MB/s eta 0:00:01\r\u001b[K     |█████████████████████▍          | 839kB 11.0MB/s eta 0:00:01\r\u001b[K     |█████████████████████▋          | 849kB 11.0MB/s eta 0:00:01\r\u001b[K     |██████████████████████          | 860kB 11.0MB/s eta 0:00:01\r\u001b[K     |██████████████████████▏         | 870kB 11.0MB/s eta 0:00:01\r\u001b[K     |██████████████████████▍         | 880kB 11.0MB/s eta 0:00:01\r\u001b[K     |██████████████████████▊         | 890kB 11.0MB/s eta 0:00:01\r\u001b[K     |███████████████████████         | 901kB 11.0MB/s eta 0:00:01\r\u001b[K     |███████████████████████▏        | 911kB 11.0MB/s eta 0:00:01\r\u001b[K     |███████████████████████▌        | 921kB 11.0MB/s eta 0:00:01\r\u001b[K     |███████████████████████▊        | 931kB 11.0MB/s eta 0:00:01\r\u001b[K     |████████████████████████        | 942kB 11.0MB/s eta 0:00:01\r\u001b[K     |████████████████████████▎       | 952kB 11.0MB/s eta 0:00:01\r\u001b[K     |████████████████████████▌       | 962kB 11.0MB/s eta 0:00:01\r\u001b[K     |████████████████████████▊       | 972kB 11.0MB/s eta 0:00:01\r\u001b[K     |█████████████████████████       | 983kB 11.0MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▎      | 993kB 11.0MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▌      | 1.0MB 11.0MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▉      | 1.0MB 11.0MB/s eta 0:00:01\r\u001b[K     |██████████████████████████      | 1.0MB 11.0MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▎     | 1.0MB 11.0MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▋     | 1.0MB 11.0MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▉     | 1.1MB 11.0MB/s eta 0:00:01\r\u001b[K     |███████████████████████████     | 1.1MB 11.0MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▍    | 1.1MB 11.0MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▋    | 1.1MB 11.0MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▉    | 1.1MB 11.0MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▏   | 1.1MB 11.0MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▍   | 1.1MB 11.0MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▊   | 1.1MB 11.0MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████   | 1.1MB 11.0MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▏  | 1.1MB 11.0MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▌  | 1.2MB 11.0MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▊  | 1.2MB 11.0MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████  | 1.2MB 11.0MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▎ | 1.2MB 11.0MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▌ | 1.2MB 11.0MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▊ | 1.2MB 11.0MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████ | 1.2MB 11.0MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▎| 1.2MB 11.0MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▌| 1.2MB 11.0MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▉| 1.2MB 11.0MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 1.3MB 11.0MB/s \n","\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.6/dist-packages (from transformers) (4.41.1)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.6/dist-packages (from transformers) (2019.12.20)\n","Requirement already satisfied: dataclasses; python_version < \"3.7\" in /usr/local/lib/python3.6/dist-packages (from transformers) (0.7)\n","Collecting tokenizers==0.9.2\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/7c/a5/78be1a55b2ac8d6a956f0a211d372726e2b1dd2666bb537fea9b03abd62c/tokenizers-0.9.2-cp36-cp36m-manylinux1_x86_64.whl (2.9MB)\n","\u001b[K     |████████████████████████████████| 2.9MB 49.9MB/s \n","\u001b[?25hCollecting sacremoses\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/7d/34/09d19aff26edcc8eb2a01bed8e98f13a1537005d31e95233fd48216eed10/sacremoses-0.0.43.tar.gz (883kB)\n","\u001b[K     |████████████████████████████████| 890kB 48.6MB/s \n","\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from transformers) (2.23.0)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.6/dist-packages (from transformers) (20.4)\n","Requirement already satisfied: protobuf in /usr/local/lib/python3.6/dist-packages (from transformers) (3.12.4)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from transformers) (1.18.5)\n","Collecting sentencepiece!=0.1.92\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/e5/2d/6d4ca4bef9a67070fa1cac508606328329152b1df10bdf31fb6e4e727894/sentencepiece-0.1.94-cp36-cp36m-manylinux2014_x86_64.whl (1.1MB)\n","\u001b[K     |████████████████████████████████| 1.1MB 24.9MB/s \n","\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.6/dist-packages (from transformers) (3.0.12)\n","Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (1.15.0)\n","Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (7.1.2)\n","Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (0.17.0)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2020.6.20)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (1.24.3)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2.10)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (3.0.4)\n","Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from packaging->transformers) (2.4.7)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from protobuf->transformers) (50.3.2)\n","Building wheels for collected packages: sacremoses\n","  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for sacremoses: filename=sacremoses-0.0.43-cp36-none-any.whl size=893257 sha256=3c1c6db577b58d553d8bd1f5267fd3cdd6bfcadf72e980a9effbe3357f4084a2\n","  Stored in directory: /root/.cache/pip/wheels/29/3c/fd/7ce5c3f0666dab31a50123635e6fb5e19ceb42ce38d4e58f45\n","Successfully built sacremoses\n","Installing collected packages: tokenizers, sacremoses, sentencepiece, transformers\n","Successfully installed sacremoses-0.0.43 sentencepiece-0.1.94 tokenizers-0.9.2 transformers-3.4.0\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"GeBEtBjJNOc7"},"source":["### Importing the libraries needed"]},{"cell_type":"code","metadata":{"id":"BKCDvnnamc52"},"source":["import pandas as pd\n","import torch\n","import os\n","import transformers\n","from torch.utils.data import Dataset, DataLoader\n","from transformers import DistilBertModel, DistilBertTokenizer"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"sbwUtAWdmtKt"},"source":["from torch import cuda\n","device = 'cuda' if cuda.is_available() else 'cpu'"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"JGkvSlF_muDE","colab":{"base_uri":"https://localhost:8080/","height":66,"referenced_widgets":["d1aa42091cde42539d404e480c82d429","6af1354fd22c4484bb3e73d2a26c1c69","450a8515e3b3474d88763ee6ed4c3796","18ff56c88f3a4daa8af41bc5d0dd7ba2","4848994144984a99894c9fbafad80a6f","ff6b0fbb29f040258d884ee1b5b8caef","5f8693fd3f904900b2de98d8f354ab99","b370280b34604d5296073db394a03432"]},"outputId":"b2c4c2f2-f1bf-4ffb-ca0f-ad164dbe8be1"},"source":["###  define epochs, learning rtaes and validation batch size\n","MAX_LEN = 512\n","TRAIN_BATCH_SIZE = 8\n","VALID_BATCH_SIZE = 2\n","EPOCHS = 5\n","LEARNING_RATE = 1e-05\n","### load Distilledbert tokenizer\n","tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-cased')"],"execution_count":null,"outputs":[{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"d1aa42091cde42539d404e480c82d429","version_minor":0,"version_major":2},"text/plain":["HBox(children=(FloatProgress(value=0.0, description='Downloading', max=213450.0, style=ProgressStyle(descripti…"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"OBjCrey-nc7s"},"source":["import pickle\n","##load pre-processed data\n","with open('/content/gdrive/My Drive/upwork/train_content_new.pkl','rb') as f:\n","  train_content = pickle.load(f)\n","with open('/content/gdrive/My Drive/upwork/train_rating_new.pkl','rb') as f:\n","  train_rating = pickle.load(f)\n","with open('/content/gdrive/My Drive/upwork/test_content_new.pkl','rb') as f:\n","  test_content = pickle.load(f)\n","with open('/content/gdrive/My Drive/upwork/test_rating_new.pkl','rb') as f:\n","  test_rating = pickle.load(f)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"uLTkMjMbSj2P"},"source":["**Preparing the Dataset and Dataloader**\n","\n","\n","\n","We will start with defining few key variables that will be used later during the training/fine tuning stage. Followed by creation of CustomDataset class - This defines how the text is pre-processed before sending it to the neural network. We will also define the Dataloader that will feed the data in batches to the neural network for suitable training and processing. Dataset and Dataloader are constructs of the PyTorch library for defining and controlling the data pre-processing and its passage to neural network"]},{"cell_type":"markdown","metadata":{"id":"2_2LeMThThVX"},"source":["**Dataloader**\n","\n","\n","* Dataloader is used to for creating training and validation dataloader that load data to the neural network in a defined manner. This is needed because all the data from the dataset cannot be loaded to the memory at once, hence the amount of dataloaded to the memory and then passed to the neural network needs to be controlled.\n","* This control is achieved using the parameters such as batch_size and max_len.\n","* Training and Validation dataloaders are used in the training and validation part of the flow respectively"]},{"cell_type":"code","metadata":{"id":"O94HbKZInWHO"},"source":["class DistillationBertTrain(Dataset):\n","    def __init__(self,type,tokenizer, max_len):\n","        self.len = len(train_content)\n","        self.tokenizer = tokenizer\n","        self.max_len = max_len\n","        if type == \"train\":\n","            self.data = train_content\n","            self.rating = train_rating\n","        else:\n","            self.data = test_content\n","            self.rating = test_rating\n","    def __getitem__(self, index):\n","        content = self.data[index]\n","        output = self.rating[index]\n","        inputs = self.tokenizer.encode_plus(\n","            content,\n","            None,\n","            add_special_tokens=True,\n","            max_length=self.max_len,\n","            pad_to_max_length=True,\n","            return_token_type_ids=True,\n","            truncation=True\n","        )\n","        ids = inputs['input_ids']\n","        mask = inputs['attention_mask']\n","\n","        return {\n","            'ids': torch.tensor(ids, dtype=torch.long),\n","            'mask': torch.tensor(mask, dtype=torch.long),\n","            'targets': torch.tensor(output, dtype=torch.long)\n","        } \n","    \n","    def __len__(self):\n","        return self.len"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"hMeZy4S5oyuP"},"source":["training_set = DistillationBertTrain('train', tokenizer, MAX_LEN)\n","testing_set = DistillationBertTrain('test', tokenizer, MAX_LEN)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ToyfWnRfpioA"},"source":["train_params = {'batch_size': TRAIN_BATCH_SIZE,\n","                'shuffle': True,\n","                'num_workers': 0\n","                }\n","\n","test_params = {'batch_size': VALID_BATCH_SIZE,\n","                'shuffle': True,\n","                'num_workers': 0\n","                }\n","\n","training_loader = DataLoader(training_set, **train_params)\n","testing_loader = DataLoader(testing_set, **test_params)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"1wjqcH0UU0h4"},"source":["**Creating the Neural Network for Fine Tuning**\n","\n","\n","**Neural Network**\n","* We will be creating a neural network with the DistillBERTClass.\n","* This network will have the DistilBERT Language model followed by a dropout and finally a Linear layer to obtain the final outputs.\n","* The data will be fed to the DistilBERT Language model as defined in the dataset.\n","* Final layer outputs is what will be compared to the rating to determine the accuracy of models prediction.\n","* We will initiate an instance of the network called model. This instance will be used for training and then to save the final trained model for future inference.\n","\n","**Loss Function and Optimizer**\n","\n","\n","* Loss Function and Optimizer and defined in the next cell.\n","* The Loss Function is used the calculate the difference in the output created by the model and the actual output.\n","* Optimizer is used to update the weights of the neural network to improve its performance."]},{"cell_type":"code","metadata":{"id":"QxiDKttoqmEq"},"source":["class DistillBERTClass(torch.nn.Module):\n","    def __init__(self):\n","        super(DistillBERTClass, self).__init__()\n","        self.l1 = DistilBertModel.from_pretrained(\"distilbert-base-uncased\")\n","        self.pre_classifier = torch.nn.Linear(768, 768)\n","        self.dropout = torch.nn.Dropout(0.3)\n","        self.classifier = torch.nn.Linear(768, 6)\n","\n","    def forward(self, input_ids, attention_mask):\n","        output_1 = self.l1(input_ids=input_ids, attention_mask=attention_mask)\n","        hidden_state = output_1[0]\n","        pooler = hidden_state[:, 0]\n","        pooler = self.pre_classifier(pooler)\n","        pooler = torch.nn.ReLU()(pooler)\n","        pooler = self.dropout(pooler)\n","        output = self.classifier(pooler)\n","        return output"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"A-q91Q1wnN13"},"source":["### store the trained model output_dir\n","output_dir = '/content/gdrive/My Drive/upwork/state_dict_model.pt'\n","model = DistillBERTClass()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"qNG-lcJoGaUn","colab":{"base_uri":"https://localhost:8080/"},"outputId":"8c37d1af-c460-43aa-84dc-97620f23c12a"},"source":["model.load_state_dict(torch.load(output_dir))\n","model.to(device)"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["DistillBERTClass(\n","  (l1): DistilBertModel(\n","    (embeddings): Embeddings(\n","      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n","      (position_embeddings): Embedding(512, 768)\n","      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","      (dropout): Dropout(p=0.1, inplace=False)\n","    )\n","    (transformer): Transformer(\n","      (layer): ModuleList(\n","        (0): TransformerBlock(\n","          (attention): MultiHeadSelfAttention(\n","            (dropout): Dropout(p=0.1, inplace=False)\n","            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n","            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n","            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n","            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n","          )\n","          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","          (ffn): FFN(\n","            (dropout): Dropout(p=0.1, inplace=False)\n","            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n","            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n","          )\n","          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","        )\n","        (1): TransformerBlock(\n","          (attention): MultiHeadSelfAttention(\n","            (dropout): Dropout(p=0.1, inplace=False)\n","            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n","            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n","            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n","            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n","          )\n","          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","          (ffn): FFN(\n","            (dropout): Dropout(p=0.1, inplace=False)\n","            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n","            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n","          )\n","          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","        )\n","        (2): TransformerBlock(\n","          (attention): MultiHeadSelfAttention(\n","            (dropout): Dropout(p=0.1, inplace=False)\n","            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n","            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n","            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n","            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n","          )\n","          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","          (ffn): FFN(\n","            (dropout): Dropout(p=0.1, inplace=False)\n","            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n","            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n","          )\n","          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","        )\n","        (3): TransformerBlock(\n","          (attention): MultiHeadSelfAttention(\n","            (dropout): Dropout(p=0.1, inplace=False)\n","            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n","            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n","            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n","            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n","          )\n","          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","          (ffn): FFN(\n","            (dropout): Dropout(p=0.1, inplace=False)\n","            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n","            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n","          )\n","          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","        )\n","        (4): TransformerBlock(\n","          (attention): MultiHeadSelfAttention(\n","            (dropout): Dropout(p=0.1, inplace=False)\n","            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n","            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n","            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n","            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n","          )\n","          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","          (ffn): FFN(\n","            (dropout): Dropout(p=0.1, inplace=False)\n","            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n","            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n","          )\n","          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","        )\n","        (5): TransformerBlock(\n","          (attention): MultiHeadSelfAttention(\n","            (dropout): Dropout(p=0.1, inplace=False)\n","            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n","            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n","            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n","            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n","          )\n","          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","          (ffn): FFN(\n","            (dropout): Dropout(p=0.1, inplace=False)\n","            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n","            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n","          )\n","          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","        )\n","      )\n","    )\n","  )\n","  (pre_classifier): Linear(in_features=768, out_features=768, bias=True)\n","  (dropout): Dropout(p=0.3, inplace=False)\n","  (classifier): Linear(in_features=768, out_features=6, bias=True)\n",")"]},"metadata":{"tags":[]},"execution_count":13}]},{"cell_type":"code","metadata":{"id":"feqBABXFGgX1"},"source":["loss_function = torch.nn.CrossEntropyLoss()\n","optimizer = torch.optim.Adam(params =  model.parameters(), lr=LEARNING_RATE)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"URFz1DD6nfmm"},"source":["### load the optimizer to cintinue training\n","optimizer.load_state_dict(torch.load('/content/gdrive/My Drive/upwork/optimizer.pt'))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"UjxAWwHtGiPV"},"source":["def calcuate_accu(big_idx, targets):\n","    n_correct = (big_idx==targets).sum().item()\n","    return n_correct"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"guBk4Fs4Gk84"},"source":["def train(epoch):\n","    tr_loss = 0\n","    n_correct = 0\n","    nb_tr_steps = 0\n","    nb_tr_examples = 0\n","    model.train()\n","    for i,data in enumerate(training_loader, 0):\n","        ids = data['ids'].to(device, dtype = torch.long)\n","        mask = data['mask'].to(device, dtype = torch.long)\n","        targets = data['targets'].to(device, dtype = torch.long)\n","\n","        outputs = model(ids, mask)\n","        loss = loss_function(outputs, targets)\n","        tr_loss += loss.item()\n","        big_val, big_idx = torch.max(outputs.data, dim=1)\n","        n_correct += calcuate_accu(big_idx, targets)\n","\n","        nb_tr_steps += 1\n","        nb_tr_examples+=targets.size(0)\n","        \n","        if i%500==0:\n","            loss_step = tr_loss/nb_tr_steps\n","            accu_step = (n_correct*100)/nb_tr_examples \n","            print(f\"Training Loss per {i} steps: {loss_step}\")\n","            print(f\"Training Accuracy per {i} steps: {accu_step}\")\n","\n","        optimizer.zero_grad()\n","        loss.backward()\n","        # # When using GPU\n","        optimizer.step()\n","    torch.save(model.state_dict(), output_dir)\n","    torch.save(optimizer.state_dict(), os.path.join('/content/gdrive/My Drive/upwork', 'optimizer.pt'))\n","    print(f'The Total Accuracy for Epoch {epoch}: {(n_correct*100)/nb_tr_examples}')\n","    epoch_loss = tr_loss/nb_tr_steps\n","    epoch_accu = (n_correct*100)/nb_tr_examples\n","    print(f\"Training Loss {epoch}: {epoch_loss}\")\n","    print(f\"Training Accuracy {epoch}: {epoch_accu}\")\n","\n","    return"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"bbxbruxVVqMo"},"source":["**Fine Tuning the Model**  \n","\n","\n","\n","After all the effort of loading and preparing the data and datasets, creating the model and defining its loss and optimizer. This is probably the easier steps in the process.\n","\n","Here we define a training function that trains the model on the training dataset created above, specified number of times (EPOCH), An epoch defines how many times the complete data will be passed through the network.\n","\n","Following events happen in this function to fine tune the neural network:\n","\n","* The dataloader passes data to the model based on the batch size.\n","* Subsequent output from the model and the actual category are compared to calculate the loss.\n","* Loss value is used to optimize the weights of the neurons in the network.\n","* After every 500 steps the loss value is printed in the console."]},{"cell_type":"code","metadata":{"id":"_sZEGAkOHcXr","colab":{"base_uri":"https://localhost:8080/"},"outputId":"78ec61de-ca9d-4b62-f3a4-e9f4bdbbee84"},"source":["for epoch in range(EPOCHS):\n","    train(epoch)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:1944: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"],"name":"stderr"},{"output_type":"stream","text":["Training Loss per 0 steps: 0.5905399918556213\n","Training Accuracy per 0 steps: 62.5\n","Training Loss per 500 steps: 0.6440870208059718\n","Training Accuracy per 500 steps: 71.8313373253493\n","Training Loss per 1000 steps: 0.6557453692435742\n","Training Accuracy per 1000 steps: 71.71578421578421\n","Training Loss per 1500 steps: 0.6605779838653345\n","Training Accuracy per 1500 steps: 71.9020652898068\n","Training Loss per 2000 steps: 0.6621654126821072\n","Training Accuracy per 2000 steps: 71.82033983008496\n","Training Loss per 2500 steps: 0.663321487769419\n","Training Accuracy per 2500 steps: 71.67133146741304\n","Training Loss per 3000 steps: 0.661705512487662\n","Training Accuracy per 3000 steps: 71.71359546817727\n","Training Loss per 3500 steps: 0.6598928182363782\n","Training Accuracy per 3500 steps: 71.76163953156241\n","Training Loss per 4000 steps: 0.6602719345336495\n","Training Accuracy per 4000 steps: 71.8507873031742\n","Training Loss per 4500 steps: 0.6613999477984296\n","Training Accuracy per 4500 steps: 71.76738502554988\n","Training Loss per 5000 steps: 0.6594160955605615\n","Training Accuracy per 5000 steps: 71.78314337132574\n","Training Loss per 5500 steps: 0.6593806431864938\n","Training Accuracy per 5500 steps: 71.77104162879476\n","Training Loss per 6000 steps: 0.6588720561558754\n","Training Accuracy per 6000 steps: 71.78803532744543\n","Training Loss per 6500 steps: 0.6599936980311182\n","Training Accuracy per 6500 steps: 71.66589755422243\n","Training Loss per 7000 steps: 0.6594738259575245\n","Training Accuracy per 7000 steps: 71.7611769747179\n","Training Loss per 7500 steps: 0.6584586308504992\n","Training Accuracy per 7500 steps: 71.85208638848154\n","Training Loss per 8000 steps: 0.6575150490794207\n","Training Accuracy per 8000 steps: 71.90351206099237\n","Training Loss per 8500 steps: 0.6580631366029598\n","Training Accuracy per 8500 steps: 71.85625220562287\n","Training Loss per 9000 steps: 0.6568393894082082\n","Training Accuracy per 9000 steps: 71.9003444061771\n","Training Loss per 9500 steps: 0.6571283713853582\n","Training Accuracy per 9500 steps: 71.93058625407852\n","Training Loss per 10000 steps: 0.6576444800600816\n","Training Accuracy per 10000 steps: 71.88906109389062\n","Training Loss per 10500 steps: 0.6582839794314347\n","Training Accuracy per 10500 steps: 71.8895819445767\n","Training Loss per 11000 steps: 0.6589750309671427\n","Training Accuracy per 11000 steps: 71.86960276338515\n","Training Loss per 11500 steps: 0.6600183994718121\n","Training Accuracy per 11500 steps: 71.83505782105904\n","Training Loss per 12000 steps: 0.6597057971648599\n","Training Accuracy per 12000 steps: 71.80443296391968\n","Training Loss per 12500 steps: 0.6600665671593112\n","Training Accuracy per 12500 steps: 71.81625469962403\n","Training Loss per 13000 steps: 0.6600983785123662\n","Training Accuracy per 13000 steps: 71.82716714098916\n","Training Loss per 13500 steps: 0.6596338090967421\n","Training Accuracy per 13500 steps: 71.86504703355307\n","Training Loss per 14000 steps: 0.6603246297934474\n","Training Accuracy per 14000 steps: 71.84933219055782\n","Training Loss per 14500 steps: 0.6604626562866899\n","Training Accuracy per 14500 steps: 71.82349493138405\n","Training Loss per 15000 steps: 0.6600960077293618\n","Training Accuracy per 15000 steps: 71.83771081927871\n","Training Loss per 15500 steps: 0.6600643596386311\n","Training Accuracy per 15500 steps: 71.84617121476033\n","Training Loss per 16000 steps: 0.6607153086869108\n","Training Accuracy per 16000 steps: 71.81191800512468\n","Training Loss per 16500 steps: 0.6610605440066226\n","Training Accuracy per 16500 steps: 71.80398157687412\n","Training Loss per 17000 steps: 0.6615410504513057\n","Training Accuracy per 17000 steps: 71.78327745426739\n","Training Loss per 17500 steps: 0.6614810163126994\n","Training Accuracy per 17500 steps: 71.7901834180904\n","Training Loss per 18000 steps: 0.6619577519697029\n","Training Accuracy per 18000 steps: 71.76406866285207\n","Training Loss per 18500 steps: 0.6622304053111603\n","Training Accuracy per 18500 steps: 71.75017566618021\n","Training Loss per 19000 steps: 0.6623973107235688\n","Training Accuracy per 19000 steps: 71.73504026103889\n","Training Loss per 19500 steps: 0.6627746402283557\n","Training Accuracy per 19500 steps: 71.71555304856162\n","Training Loss per 20000 steps: 0.6623187176013409\n","Training Accuracy per 20000 steps: 71.7520373981301\n","Training Loss per 20500 steps: 0.6624214275766481\n","Training Accuracy per 20500 steps: 71.76479196136773\n","Training Loss per 21000 steps: 0.6625597818981358\n","Training Accuracy per 21000 steps: 71.75313080329508\n","Training Loss per 21500 steps: 0.6624970663373378\n","Training Accuracy per 21500 steps: 71.7501511557602\n","Training Loss per 22000 steps: 0.6629020108344268\n","Training Accuracy per 22000 steps: 71.7166265169765\n","Training Loss per 22500 steps: 0.6631739283164583\n","Training Accuracy per 22500 steps: 71.7045909070708\n","Training Loss per 23000 steps: 0.6636442567096016\n","Training Accuracy per 23000 steps: 71.66862310334334\n","Training Loss per 23500 steps: 0.6635811652628839\n","Training Accuracy per 23500 steps: 71.66982256074209\n","Training Loss per 24000 steps: 0.6636356366452543\n","Training Accuracy per 24000 steps: 71.64545227282197\n","Training Loss per 24500 steps: 0.6637836918329094\n","Training Accuracy per 24500 steps: 71.62462756622179\n","Training Loss per 25000 steps: 0.6638212439736234\n","Training Accuracy per 25000 steps: 71.63513459461622\n","Training Loss per 25500 steps: 0.6637210507441219\n","Training Accuracy per 25500 steps: 71.63297517744402\n","Training Loss per 26000 steps: 0.6636300037556724\n","Training Accuracy per 26000 steps: 71.64291758009307\n","Training Loss per 26500 steps: 0.6639155538427322\n","Training Accuracy per 26500 steps: 71.62088223085921\n","Training Loss per 27000 steps: 0.6639714527616218\n","Training Accuracy per 27000 steps: 71.60892189178179\n","Training Loss per 27500 steps: 0.6638974281483262\n","Training Accuracy per 27500 steps: 71.62375913603142\n","Training Loss per 28000 steps: 0.6639852936748036\n","Training Accuracy per 28000 steps: 71.62020999250026\n","Training Loss per 28500 steps: 0.6636884033163407\n","Training Accuracy per 28500 steps: 71.64266166099435\n","Training Loss per 29000 steps: 0.6638962124259155\n","Training Accuracy per 29000 steps: 71.63718492465777\n","Training Loss per 29500 steps: 0.6637171664858975\n","Training Accuracy per 29500 steps: 71.66536727568557\n","Training Loss per 30000 steps: 0.6640594814869607\n","Training Accuracy per 30000 steps: 71.63677877404086\n","Training Loss per 30500 steps: 0.6641458354868117\n","Training Accuracy per 30500 steps: 71.63330710468509\n","Training Loss per 31000 steps: 0.6644294718503645\n","Training Accuracy per 31000 steps: 71.62954420825135\n","Training Loss per 31500 steps: 0.6646682968360387\n","Training Accuracy per 31500 steps: 71.59852068188312\n","Training Loss per 32000 steps: 0.6645934489283807\n","Training Accuracy per 32000 steps: 71.59659073153964\n","Training Loss per 32500 steps: 0.6645047199492828\n","Training Accuracy per 32500 steps: 71.59241254115258\n","Training Loss per 33000 steps: 0.6646166182416652\n","Training Accuracy per 33000 steps: 71.58495197115239\n","Training Loss per 33500 steps: 0.6648075263598638\n","Training Accuracy per 33500 steps: 71.58741530103579\n","Training Loss per 34000 steps: 0.665034578563059\n","Training Accuracy per 34000 steps: 71.58171818475927\n","Training Loss per 34500 steps: 0.6654705589416943\n","Training Accuracy per 34500 steps: 71.56567925567374\n","Training Loss per 35000 steps: 0.6653802605469681\n","Training Accuracy per 35000 steps: 71.56402674209308\n","Training Loss per 35500 steps: 0.6655298961790135\n","Training Accuracy per 35500 steps: 71.56770231824456\n","Training Loss per 36000 steps: 0.6657100355495847\n","Training Accuracy per 36000 steps: 71.54835976778423\n","Training Loss per 36500 steps: 0.6657180557849195\n","Training Accuracy per 36500 steps: 71.55488890715323\n","Training Loss per 37000 steps: 0.6655828340162745\n","Training Accuracy per 37000 steps: 71.55853895840653\n","Training Loss per 37500 steps: 0.6653416218811606\n","Training Accuracy per 37500 steps: 71.57509133089785\n","Training Loss per 38000 steps: 0.6652436884436895\n","Training Accuracy per 38000 steps: 71.57278755822215\n","Training Loss per 38500 steps: 0.6652715345365915\n","Training Accuracy per 38500 steps: 71.57379029116127\n","Training Loss per 39000 steps: 0.6651466597258557\n","Training Accuracy per 39000 steps: 71.58726699315402\n","Training Loss per 39500 steps: 0.6651819827597291\n","Training Accuracy per 39500 steps: 71.57730184045974\n","Training Loss per 40000 steps: 0.6652428532450775\n","Training Accuracy per 40000 steps: 71.56789830254243\n","Training Loss per 40500 steps: 0.6653113402123628\n","Training Accuracy per 40500 steps: 71.56397372904372\n","Training Loss per 41000 steps: 0.6652753329953608\n","Training Accuracy per 41000 steps: 71.57447379332211\n","Training Loss per 41500 steps: 0.6652922640295391\n","Training Accuracy per 41500 steps: 71.58622683790752\n","Training Loss per 42000 steps: 0.665276562734952\n","Training Accuracy per 42000 steps: 71.57895050117854\n","Training Loss per 42500 steps: 0.6654117589290584\n","Training Accuracy per 42500 steps: 71.57566880779275\n","Training Loss per 43000 steps: 0.6656632928781068\n","Training Accuracy per 43000 steps: 71.56548684914304\n","Training Loss per 43500 steps: 0.6655374968859089\n","Training Accuracy per 43500 steps: 71.56961908921635\n","Training Loss per 44000 steps: 0.6658801291442639\n","Training Accuracy per 44000 steps: 71.55576009636145\n","Training Loss per 44500 steps: 0.6657274533087868\n","Training Accuracy per 44500 steps: 71.5520437742972\n","Training Loss per 45000 steps: 0.665832688522226\n","Training Accuracy per 45000 steps: 71.5478544921224\n","Training Loss per 45500 steps: 0.6659039583438119\n","Training Accuracy per 45500 steps: 71.54815278785081\n","Training Loss per 46000 steps: 0.6661490661802868\n","Training Accuracy per 46000 steps: 71.54790113258407\n","Training Loss per 46500 steps: 0.6662939052049612\n","Training Accuracy per 46500 steps: 71.54792370056558\n","Training Loss per 47000 steps: 0.6661195903683603\n","Training Accuracy per 47000 steps: 71.55060530627009\n","Training Loss per 47500 steps: 0.6660775356641712\n","Training Accuracy per 47500 steps: 71.55744089598113\n","Training Loss per 48000 steps: 0.6661313678947047\n","Training Accuracy per 48000 steps: 71.55814462198704\n","Training Loss per 48500 steps: 0.6660096952715293\n","Training Accuracy per 48500 steps: 71.56527700459785\n","Training Loss per 49000 steps: 0.6659630330358193\n","Training Accuracy per 49000 steps: 71.56894757249852\n","Training Loss per 49500 steps: 0.6659976954945497\n","Training Accuracy per 49500 steps: 71.56370578372155\n","Training Loss per 50000 steps: 0.6660184272178847\n","Training Accuracy per 50000 steps: 71.55331893362133\n","Training Loss per 50500 steps: 0.6662018938709922\n","Training Accuracy per 50500 steps: 71.55551375220293\n","Training Loss per 51000 steps: 0.666152908517759\n","Training Accuracy per 51000 steps: 71.55717534950296\n","Training Loss per 51500 steps: 0.6660610538227244\n","Training Accuracy per 51500 steps: 71.56341624434477\n","Training Loss per 52000 steps: 0.6662220314062847\n","Training Accuracy per 52000 steps: 71.55198938481952\n","Training Loss per 52500 steps: 0.6663339514395947\n","Training Accuracy per 52500 steps: 71.53554218014895\n","Training Loss per 53000 steps: 0.6662330554792587\n","Training Accuracy per 53000 steps: 71.54534820097734\n","Training Loss per 53500 steps: 0.6662941745898026\n","Training Accuracy per 53500 steps: 71.54282162950226\n","Training Loss per 54000 steps: 0.6663836691830647\n","Training Accuracy per 54000 steps: 71.53362900687024\n","Training Loss per 54500 steps: 0.6663650436968453\n","Training Accuracy per 54500 steps: 71.5289627713253\n","Training Loss per 55000 steps: 0.6663198094197373\n","Training Accuracy per 55000 steps: 71.52892674678642\n","Training Loss per 55500 steps: 0.6663043335579797\n","Training Accuracy per 55500 steps: 71.52663915965478\n","Training Loss per 56000 steps: 0.6663423721942963\n","Training Accuracy per 56000 steps: 71.51367832717273\n","Training Loss per 56500 steps: 0.6663331529274086\n","Training Accuracy per 56500 steps: 71.51289357710483\n","Training Loss per 57000 steps: 0.6663751347602743\n","Training Accuracy per 57000 steps: 71.50751741197523\n","The Total Accuracy for Epoch 0: 71.51610636617161\n","Training Loss 0: 0.6663271066881693\n","Training Accuracy 0: 71.51610636617161\n","Training Loss per 0 steps: 0.7672868371009827\n","Training Accuracy per 0 steps: 50.0\n","Training Loss per 500 steps: 0.575814982612214\n","Training Accuracy per 500 steps: 75.39920159680639\n","Training Loss per 1000 steps: 0.5771591760046951\n","Training Accuracy per 1000 steps: 75.4995004995005\n","Training Loss per 1500 steps: 0.5834557460168057\n","Training Accuracy per 1500 steps: 75.34143904063957\n","Training Loss per 2000 steps: 0.5845708246791976\n","Training Accuracy per 2000 steps: 75.26861569215393\n","Training Loss per 2500 steps: 0.5801719371877352\n","Training Accuracy per 2500 steps: 75.41483406637344\n","Training Loss per 3000 steps: 0.580733213455766\n","Training Accuracy per 3000 steps: 75.48317227590803\n","Training Loss per 3500 steps: 0.5824558649236595\n","Training Accuracy per 3500 steps: 75.43916023993145\n","Training Loss per 4000 steps: 0.5861378314568978\n","Training Accuracy per 4000 steps: 75.27805548612847\n","Training Loss per 4500 steps: 0.5862660731833078\n","Training Accuracy per 4500 steps: 75.26660742057321\n","Training Loss per 5000 steps: 0.5857565021025041\n","Training Accuracy per 5000 steps: 75.25744851029793\n","Training Loss per 5500 steps: 0.5857308840880587\n","Training Accuracy per 5500 steps: 75.32039629158335\n","Training Loss per 6000 steps: 0.5850850787386856\n","Training Accuracy per 6000 steps: 75.41243126145642\n","Training Loss per 6500 steps: 0.5868998691565696\n","Training Accuracy per 6500 steps: 75.31918166435933\n","Training Loss per 7000 steps: 0.5872293030610342\n","Training Accuracy per 7000 steps: 75.30709898585916\n","Training Loss per 7500 steps: 0.5872069112365523\n","Training Accuracy per 7500 steps: 75.31662445007332\n","Training Loss per 8000 steps: 0.58731734163574\n","Training Accuracy per 8000 steps: 75.35151856017998\n","Training Loss per 8500 steps: 0.5866433692029639\n","Training Accuracy per 8500 steps: 75.3558404893542\n","Training Loss per 9000 steps: 0.5862531316269107\n","Training Accuracy per 9000 steps: 75.37912454171759\n","Training Loss per 9500 steps: 0.5862365738059292\n","Training Accuracy per 9500 steps: 75.37364487948636\n","Training Loss per 10000 steps: 0.587042024283305\n","Training Accuracy per 10000 steps: 75.32996700329967\n","Training Loss per 10500 steps: 0.5877139381237761\n","Training Accuracy per 10500 steps: 75.29878106846967\n","Training Loss per 11000 steps: 0.5873827549294768\n","Training Accuracy per 11000 steps: 75.32042541587128\n","Training Loss per 11500 steps: 0.5873001778310262\n","Training Accuracy per 11500 steps: 75.36083818798366\n","Training Loss per 12000 steps: 0.5871053493556336\n","Training Accuracy per 12000 steps: 75.38121823181402\n","Training Loss per 12500 steps: 0.5879698058846722\n","Training Accuracy per 12500 steps: 75.372970162387\n","Training Loss per 13000 steps: 0.5895925416915511\n","Training Accuracy per 13000 steps: 75.2894008153219\n","Training Loss per 13500 steps: 0.5895672394669804\n","Training Accuracy per 13500 steps: 75.28794163395304\n","Training Loss per 14000 steps: 0.5892190625795968\n","Training Accuracy per 14000 steps: 75.3544389686451\n","Training Loss per 14500 steps: 0.5899023373109227\n","Training Accuracy per 14500 steps: 75.33014964485209\n","Training Loss per 15000 steps: 0.5894517504400464\n","Training Accuracy per 15000 steps: 75.33831077928139\n","Training Loss per 15500 steps: 0.5895007360301459\n","Training Accuracy per 15500 steps: 75.35400941874718\n","Training Loss per 16000 steps: 0.5901069250783133\n","Training Accuracy per 16000 steps: 75.31560527467033\n","Training Loss per 16500 steps: 0.5901602374517436\n","Training Accuracy per 16500 steps: 75.30679958790377\n","Training Loss per 17000 steps: 0.5904787859004754\n","Training Accuracy per 17000 steps: 75.29189459443562\n","Training Loss per 17500 steps: 0.5904425382465133\n","Training Accuracy per 17500 steps: 75.2835552254157\n","Training Loss per 18000 steps: 0.5904654987310776\n","Training Accuracy per 18000 steps: 75.30553858118994\n","Training Loss per 18500 steps: 0.5912205743164503\n","Training Accuracy per 18500 steps: 75.26214799200044\n","Training Loss per 19000 steps: 0.5912749535186963\n","Training Accuracy per 19000 steps: 75.2743276669649\n","Training Loss per 19500 steps: 0.5910825799470755\n","Training Accuracy per 19500 steps: 75.27947284754627\n","Training Loss per 20000 steps: 0.5911509410261327\n","Training Accuracy per 20000 steps: 75.26123693815309\n","Training Loss per 20500 steps: 0.5911474406150811\n","Training Accuracy per 20500 steps: 75.27986439685868\n","Training Loss per 21000 steps: 0.591186828312803\n","Training Accuracy per 21000 steps: 75.27915337364887\n","Training Loss per 21500 steps: 0.5916627755929279\n","Training Accuracy per 21500 steps: 75.25347658248454\n","Training Loss per 22000 steps: 0.5920134655730648\n","Training Accuracy per 22000 steps: 75.23521658106449\n","Training Loss per 22500 steps: 0.5925686759460735\n","Training Accuracy per 22500 steps: 75.21165725967735\n","Training Loss per 23000 steps: 0.5926393048594757\n","Training Accuracy per 23000 steps: 75.2162949436981\n","Training Loss per 23500 steps: 0.5926911030778667\n","Training Accuracy per 23500 steps: 75.21275690396153\n","Training Loss per 24000 steps: 0.5923604477921937\n","Training Accuracy per 24000 steps: 75.22030332069497\n","Training Loss per 24500 steps: 0.5925119591531794\n","Training Accuracy per 24500 steps: 75.20458348638832\n","Training Loss per 25000 steps: 0.5926958983600519\n","Training Accuracy per 25000 steps: 75.19999200031998\n","Training Loss per 25500 steps: 0.5927431961706814\n","Training Accuracy per 25500 steps: 75.21665817026783\n","Training Loss per 26000 steps: 0.5928822456091135\n","Training Accuracy per 26000 steps: 75.21681858390062\n","Training Loss per 26500 steps: 0.5929736024069114\n","Training Accuracy per 26500 steps: 75.21131278064979\n","Training Loss per 27000 steps: 0.5936408978003731\n","Training Accuracy per 27000 steps: 75.19212251398096\n","Training Loss per 27500 steps: 0.593668078284195\n","Training Accuracy per 27500 steps: 75.18408421511946\n","Training Loss per 28000 steps: 0.5937695380315318\n","Training Accuracy per 28000 steps: 75.18436841541373\n","Training Loss per 28500 steps: 0.5940528443479014\n","Training Accuracy per 28500 steps: 75.1758710220694\n"],"name":"stdout"}]}]}